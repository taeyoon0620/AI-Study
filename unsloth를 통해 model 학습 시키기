DeepSeek 모델을 Unsloth를 통해 로컬 파일 (CSV) 및 구름 데이터셋을 사용하여 학습시키려면, 다음과 같은 단계를 따를 수 있습니다. Unsloth는 효율적인 학습을 위해 설계된 라이브러리로, 특히 대규모 언어 모델을 빠르게 학습시키는 데 유용합니다.

1. 환경 설정
먼저 필요한 라이브러리를 설치합니다. Unsloth와 Hugging Face의 transformers, datasets 라이브러리가 필요합니다.

pip install unsloth transformers datasets
2. 모델 및 토크나이저 로드
DeepSeek 모델과 토크나이저를 로드합니다. FastLanguageModel.from_pretrained를 사용하여 모델을 로드할 수 있습니다.

from unsloth import FastLanguageModel

model_name = "deepseek-ai/deepseek-llm-7b-base"  # 예시 모델 이름
max_seq_length = 2048  # 최대 시퀀스 길이
dtype = None  # 데이터 타입 (None은 기본값)
load_in_4bit = True  # 4비트 양자화 사용 여부

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)
3. 데이터셋 준비
로컬 CSV 파일과 구름 데이터셋을 준비합니다. datasets 라이브러리를 사용하여 데이터를 로드하고 전처리할 수 있습니다.

from datasets import load_dataset, Dataset

# 로컬 CSV 파일 로드
local_dataset = Dataset.from_csv("path/to/your/local_dataset.csv")

# 구름 데이터셋 로드 (예시)
cloud_dataset = load_dataset("cloud_dataset_name")

# 데이터셋 병합 (필요한 경우)
combined_dataset = concatenate_datasets([local_dataset, cloud_dataset])
4. 데이터 전처리
토크나이저를 사용하여 데이터를 모델 입력 형식에 맞게 전처리합니다.


def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=max_seq_length)

tokenized_dataset = combined_dataset.map(preprocess_function, batched=True)
5. 학습 설정
Unsloth를 사용하여 모델을 학습시킵니다. 학습 설정을 구성하고 학습을 시작합니다.


from unsloth import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=10,
    save_steps=500,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

trainer.train()
6. 모델 저장
학습이 완료된 모델을 저장합니다.


model.save_pretrained("./trained_model")
tokenizer.save_pretrained("./trained_model")
7. 모델 평가 및 추론
학습된 모델을 평가하거나 추론에 사용할 수 있습니다.


# 평가
eval_results = trainer.evaluate()

# 추론
input_text = "Your input text here"
inputs = tokenizer(input_text, return_tensors="pt")
outputs = model.generate(**inputs)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
